{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "                                              0.0/1.5 MB 1.4 MB/s eta 0:00:02\n",
      "     --                                       0.1/1.5 MB 1.3 MB/s eta 0:00:02\n",
      "     -----                                    0.2/1.5 MB 1.5 MB/s eta 0:00:01\n",
      "     ------                                   0.3/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "     ------                                   0.3/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "     ------                                   0.3/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "     ------                                   0.3/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "     --------------------                     0.8/1.5 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------                    0.8/1.5 MB 1.9 MB/s eta 0:00:01\n",
      "     ----------------------                   0.9/1.5 MB 1.8 MB/s eta 0:00:01\n",
      "     --------------------------               1.0/1.5 MB 1.9 MB/s eta 0:00:01\n",
      "     -----------------------------            1.1/1.5 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------------------------       1.3/1.5 MB 2.1 MB/s eta 0:00:01\n",
      "     -------------------------------------    1.4/1.5 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 2.2 MB/s eta 0:00:00\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "                                              0.0/97.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 97.9/97.9 kB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\users\\90553\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\90553\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (2024.5.15)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "                                              0.0/78.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 78.3/78.3 kB 4.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\90553\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1 tqdm-4.66.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install regex\n",
    "!pip install nltk\n",
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\90553\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\90553\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\90553\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import regex\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def filter_text(text, stop_words):\n",
    "    word_tokens = WordPunctTokenizer().tokenize(text.lower())\n",
    "    filtered_text = [regex.sub('[^a-z ]+', '', w) for w in word_tokens]\n",
    "    filtered_text = [regex.sub('[ ][ ]+', '', w) for w in filtered_text]\n",
    "    filtered_text = [regex.sub('[0-9]', '', w) for w in filtered_text]\n",
    "    filtered_text = [wordnet_lemmatizer.lemmatize(w, 'v') for w in filtered_text if not w in stop_words and len(w) > 2 and len(w) < 50]\n",
    "    return ' '.join(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_data = pd.read_csv('../datasets/bugs-train.csv')\n",
    "test_data = pd.read_csv('../datasets/bugs-test.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Text and labels\n",
    "X = train_data['summary'].values\n",
    "y = train_data['severity'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the text\n",
    "X = np.array([filter_text(text, stop) for text in X])\n",
    "test_summaries = np.array([filter_text(text, stop) for text in test_data['summary'].values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=5000, lower=True, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_test = tokenizer.texts_to_sequences(test_summaries)\n",
    "\n",
    "# Pad the sequences\n",
    "max_seq_length = 100\n",
    "X_train = pad_sequences(X_train, maxlen=max_seq_length)\n",
    "X_val = pad_sequences(X_val, maxlen=max_seq_length)\n",
    "X_test = pad_sequences(X_test, maxlen=max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\90553\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 2s/step - accuracy: 0.7402 - loss: 2.5260 - val_accuracy: 0.7912 - val_loss: 1.0797\n",
      "Epoch 2/7\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 2s/step - accuracy: 0.7888 - loss: 0.9780 - val_accuracy: 0.8532 - val_loss: 0.6928\n",
      "Epoch 3/7\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 2s/step - accuracy: 0.8468 - loss: 0.6899 - val_accuracy: 0.8570 - val_loss: 0.6351\n",
      "Epoch 4/7\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 1s/step - accuracy: 0.8522 - loss: 0.6406 - val_accuracy: 0.8585 - val_loss: 0.6108\n",
      "Epoch 5/7\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 1s/step - accuracy: 0.8515 - loss: 0.6269 - val_accuracy: 0.8579 - val_loss: 0.6081\n",
      "Epoch 6/7\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1s/step - accuracy: 0.8528 - loss: 0.6184 - val_accuracy: 0.8591 - val_loss: 0.5926\n",
      "Epoch 7/7\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - accuracy: 0.8546 - loss: 0.6087 - val_accuracy: 0.8578 - val_loss: 0.5872\n",
      "\u001b[1m1000/1000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 20ms/step - accuracy: 0.8592 - loss: 0.5821\n",
      "Validation Loss: 0.5871909260749817\n",
      "Validation Accuracy: 0.8578437566757202\n",
      "\u001b[1m2691/2691\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 19ms/step\n",
      "Predictions saved to 'predicted_bugs.csv'\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import AdamW\n",
    "\n",
    "# Build the model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Build the model with L2 regularization and adjusted dropout rates\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=128, input_length=max_seq_length))\n",
    "model.add(SpatialDropout1D(0.3))  # Adjusted dropout rate\n",
    "model.add(LSTM(128, dropout=0.3, recurrent_dropout=0.3, kernel_regularizer=l2(0.01)))  # Added L2 regularization and adjusted dropout rates\n",
    "model.add(Dense(7, activation='softmax', kernel_regularizer=l2(0.01)))  # Added L2 regularization to the dense layer\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=AdamW(learning_rate=0.001, weight_decay=1e-4), metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=7, batch_size=2048, validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "results = model.evaluate(X_val, y_val, verbose=1)\n",
    "print(f\"Validation Loss: {results[0]}\")\n",
    "print(f\"Validation Accuracy: {results[1]}\")\n",
    "\n",
    "# Map predictions back to original severity labels\n",
    "severity_mapping = {'enhancement': 1, 'minor': 2, 'normal': 3, 'major': 4, 'blocker': 5, 'critical': 6, 'trivial': 7}\n",
    "inverse_severity_mapping = {v: k for k, v in severity_mapping.items()}\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = predictions.argmax(axis=1)\n",
    "\n",
    "# Correctly map predicted labels back using the label encoder\n",
    "predicted_labels_mapped = label_encoder.inverse_transform(predicted_labels)\n",
    "\n",
    "# Save the predictions\n",
    "pred_df = pd.DataFrame({\n",
    "    'bug_id': test_data['bug_id'],\n",
    "    'severity': predicted_labels_mapped\n",
    "})\n",
    "pred_df.to_csv(\"./predictions/predicted2_bugs.csv\", index=False)\n",
    "\n",
    "print(\"Predictions saved to 'predicted_bugs.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
